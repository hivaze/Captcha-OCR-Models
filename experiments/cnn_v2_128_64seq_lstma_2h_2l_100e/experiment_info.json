{
    "best_epoch": {
        "number": 95,
        "train_loss": 0.15822239305022395,
        "eval_loss": 0.17647
    },
    "history": {
        "train": [
            5.567880473559415,
            4.4206212623209895,
            4.411223321021358,
            4.398974895477295,
            4.397652312170101,
            4.395984619478636,
            4.390868307668952,
            4.3742038449154625,
            4.353803381135192,
            4.212720243236687,
            3.224787358996234,
            1.578547040118447,
            0.9175818487058712,
            0.7332934903193123,
            0.6060113280634337,
            0.5452173721941211,
            0.5089267835586886,
            0.47072842641721796,
            0.47048604412923883,
            0.4115251570562773,
            0.39791252567798274,
            0.39198562389687647,
            0.39500535138045684,
            0.38630984931052487,
            0.3741982681087301,
            0.39518088517309746,
            0.37953144689149493,
            0.3539000212014476,
            0.3103537272803391,
            0.3144017082603672,
            0.2840315859906281,
            0.2719208556639997,
            0.2718093785681302,
            0.29777519046505796,
            0.33551643958574606,
            0.2739097700466084,
            0.3019304848924468,
            0.30035619814939135,
            0.27136188236218467,
            0.2641408943300006,
            0.38967146168026745,
            0.2790695970194249,
            0.28054879398285587,
            0.24378409793105307,
            0.24407037621057487,
            0.2616668717016148,
            0.2559386786026291,
            0.25824296738527996,
            0.283613223818284,
            0.26206891219827194,
            0.24903152675568302,
            0.23381730367111253,
            0.24891794538950618,
            0.23096202559108975,
            0.2578446302987352,
            0.25248040447506725,
            0.2350375062868565,
            0.2177923206664339,
            0.20129639268675936,
            0.21264248407339748,
            0.20942348185219342,
            0.23555576933335654,
            0.25836481003067163,
            0.2479099257837368,
            0.21263059219227562,
            0.20244285973566997,
            0.19255694011344185,
            0.2063893887061107,
            0.18952017928226084,
            0.18424632051323034,
            0.2095665086673785,
            0.22746094155915175,
            0.19962939648311348,
            0.20492373350300366,
            0.20487130268276493,
            0.1835183008939405,
            0.20514764410408237,
            0.19254596908635732,
            0.23017454864103584,
            0.19268953413525713,
            0.17857057403160048,
            0.21019762433782407,
            0.2061166912317276,
            0.2521995683259602,
            0.22671769276449952,
            0.24081946758529807,
            0.22042412112785292,
            0.24921742779544637,
            0.22764694709566574,
            0.20753441241723072,
            0.18461269989043852,
            0.17140085816006118,
            0.18647852341962767,
            0.17282358934230443,
            0.1672426547053494,
            0.15822239305022395,
            0.16274405648059484,
            0.15158551742758933,
            0.15695711281857913,
            0.163827887724472
        ],
        "eval": [
            4.4385,
            4.43702,
            4.40465,
            4.40444,
            4.39714,
            4.3967,
            4.38325,
            4.38939,
            4.32456,
            3.96982,
            2.48794,
            1.10122,
            0.78698,
            0.65187,
            0.56346,
            0.50513,
            0.46562,
            0.46901,
            0.4577,
            0.38795,
            0.39527,
            0.41976,
            0.38653,
            0.38417,
            0.33582,
            0.44577,
            0.34849,
            0.34082,
            0.38299,
            0.28872,
            0.29791,
            0.27525,
            0.27606,
            0.35232,
            0.27912,
            0.32546,
            0.31135,
            0.29435,
            0.30505,
            0.43697,
            0.30021,
            0.25787,
            0.25958,
            0.24193,
            0.2616,
            0.25668,
            0.23891,
            0.26175,
            0.27882,
            0.26316,
            0.23841,
            0.23845,
            0.24002,
            0.23339,
            0.24673,
            0.2372,
            0.23035,
            0.2264,
            0.22271,
            0.21644,
            0.22549,
            0.25701,
            0.28553,
            0.21087,
            0.23103,
            0.20722,
            0.24069,
            0.21211,
            0.19362,
            0.19104,
            0.20712,
            0.20784,
            0.21187,
            0.21192,
            0.20086,
            0.20859,
            0.21133,
            0.29341,
            0.22916,
            0.2001,
            0.20594,
            0.22291,
            0.24934,
            0.24156,
            0.23422,
            0.22811,
            0.25369,
            0.28616,
            0.21556,
            0.19893,
            0.20329,
            0.19075,
            0.18787,
            0.20192,
            0.17838,
            0.17647,
            0.18236,
            0.17911,
            0.18809,
            0.17797
        ]
    },
    "architecture": "OCR_CARNN(\n  (encoder): CNNImageEncoderV2(\n    (layers): Sequential(\n      (0): ConvBlock(\n        (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (1): ConvBlock(\n        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (2): ConvBlock(\n        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (3): ConvBlock(\n        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (4): ConvBlock(\n        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (5): ConvBlock(\n        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_net): Sequential(\n      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (1): Linear(in_features=128, out_features=128, bias=True)\n    )\n  )\n  (decoder): SelfAttenBiLSTMImageDecoder(\n    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (rnn): LSTM(128, 128, num_layers=2, dropout=0.1, bidirectional=True)\n    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (self_atten): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n    )\n    (out_proj): Linear(in_features=256, out_features=65, bias=True)\n  )\n  (softmax): LogSoftmax(dim=-1)\n)"
}