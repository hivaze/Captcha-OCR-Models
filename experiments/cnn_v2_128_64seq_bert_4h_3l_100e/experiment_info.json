{
    "best_epoch": {
        "number": 99,
        "train_loss": 0.1264927929526643,
        "eval_loss": 0.15154
    },
    "history": {
        "train": [
            5.449838831454893,
            4.441414772709714,
            4.411831113356579,
            4.408952586258514,
            4.398057816903802,
            4.368298325357558,
            4.2734429564657095,
            4.023315731483169,
            2.7486218968524208,
            1.4154208611838426,
            0.9815378543696825,
            0.7896169461781466,
            0.6812367763700364,
            0.607089296926426,
            0.5595107591604884,
            0.4922310211990453,
            0.45895396003240274,
            0.43332424759864807,
            0.41694816305667537,
            0.3991320110574553,
            0.37273154756690885,
            0.34468998203549206,
            0.34043759069865265,
            0.3223423606987241,
            0.31973600632782223,
            0.3130137618583969,
            0.28833753606186635,
            0.28408937552307223,
            0.28119556700127035,
            0.272535120007358,
            0.2727369649123542,
            0.2675096053111402,
            0.25288346466384354,
            0.2480784637641303,
            0.24800774511657184,
            0.23573550393309775,
            0.23406888997252984,
            0.22381411738033535,
            0.23300182498708555,
            0.23476420852202404,
            0.22752741450750374,
            0.21857949091663845,
            0.21828089861930172,
            0.21296419052383567,
            0.20501482382982592,
            0.20959422558168822,
            0.2022802052618582,
            0.21427061550224882,
            0.2081483708152288,
            0.19835608744923072,
            0.19483275145669526,
            0.1941113951085489,
            0.18741336785539797,
            0.1845598409447489,
            0.18531777983224845,
            0.18393019577370415,
            0.18086307920232603,
            0.18177639636435086,
            0.1801722364712365,
            0.18162163535628137,
            0.1781305829180947,
            0.16953250275382512,
            0.1675583933350406,
            0.16915555854764167,
            0.16627527509309067,
            0.2179070897871935,
            0.1742534492212006,
            0.15856472006704234,
            0.16279266406840917,
            0.16340424233599554,
            0.16513254106799258,
            0.15973430919119075,
            0.15826488210807874,
            0.16888974625853043,
            0.16329026118486742,
            0.15696367338488373,
            0.1564271108636373,
            0.15503420812796942,
            0.16006842161281198,
            0.14666039638126951,
            0.1467705190747599,
            0.14191980320441572,
            0.1400907517799848,
            0.1447131704491905,
            0.14700075370978705,
            0.14439250076118904,
            0.14937899497490895,
            0.15033244916909858,
            0.1431165694058696,
            0.1390854805896554,
            0.14337806714863716,
            0.1447210385452343,
            0.13989398232366465,
            0.13350010153990757,
            0.15242056906977786,
            0.15114240291752393,
            0.13842466954566254,
            0.13536152356787573,
            0.12807307066985324,
            0.1264927929526643
        ],
        "eval": [
            4.50876,
            4.42563,
            4.41354,
            4.41455,
            4.3968,
            4.31995,
            4.22104,
            3.67806,
            1.89829,
            1.20143,
            0.88309,
            0.75272,
            0.66186,
            0.55411,
            0.52424,
            0.48225,
            0.44945,
            0.44166,
            0.4136,
            0.39407,
            0.35395,
            0.37033,
            0.36042,
            0.32554,
            0.32811,
            0.32428,
            0.29888,
            0.28875,
            0.27976,
            0.28455,
            0.28461,
            0.26268,
            0.27714,
            0.25143,
            0.25733,
            0.26193,
            0.24257,
            0.24017,
            0.2591,
            0.23488,
            0.23488,
            0.24278,
            0.23338,
            0.22443,
            0.22239,
            0.20794,
            0.21721,
            0.23137,
            0.21954,
            0.20795,
            0.2138,
            0.20198,
            0.2172,
            0.20472,
            0.20697,
            0.19373,
            0.20207,
            0.21504,
            0.1946,
            0.1921,
            0.19734,
            0.19974,
            0.1955,
            0.1891,
            0.20278,
            0.20553,
            0.18679,
            0.19169,
            0.19397,
            0.18237,
            0.18274,
            0.1783,
            0.18092,
            0.17714,
            0.17531,
            0.183,
            0.1765,
            0.17616,
            0.1795,
            0.1763,
            0.16015,
            0.17319,
            0.17333,
            0.16381,
            0.16862,
            0.16751,
            0.17279,
            0.17164,
            0.17542,
            0.16942,
            0.16664,
            0.15933,
            0.15795,
            0.15895,
            0.16441,
            0.16284,
            0.1595,
            0.15195,
            0.15707,
            0.15154
        ]
    },
    "architecture": "OCR_CNNBERT(\n  (encoder): CNNImageEncoderV2(\n    (layers): Sequential(\n      (0): ConvBlock(\n        (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (1): ConvBlock(\n        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n        (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (2): ConvBlock(\n        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (3): ConvBlock(\n        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (4): ConvBlock(\n        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n      (5): ConvBlock(\n        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n        (activation): Hardswish()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_net): Sequential(\n      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (1): Linear(in_features=128, out_features=128, bias=True)\n    )\n  )\n  (decoder): TransformerImageDecoder(\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (length_embeddings): Embedding(100, 128)\n    (model): TransformerEncoder(\n      (layers): ModuleList(\n        (0): TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n          (linear1): Linear(in_features=128, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=128, bias=True)\n          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n        (1): TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n          (linear1): Linear(in_features=128, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=128, bias=True)\n          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n        (2): TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n          )\n          (linear1): Linear(in_features=128, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=128, bias=True)\n          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (out_proj): Linear(in_features=128, out_features=65, bias=True)\n  )\n  (softmax): LogSoftmax(dim=-1)\n)"
}