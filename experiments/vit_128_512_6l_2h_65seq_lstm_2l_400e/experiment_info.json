{
    "best_epoch": {
        "number": 197,
        "train_loss": 0.6238415618486042,
        "eval_loss": 0.70024
    },
    "history": {
        "train": [
            1.1397836027266104,
            1.135824865932706,
            1.128569495828846,
            1.125587419618534,
            1.111249929741968,
            1.104564044294478,
            1.1094773547558845,
            1.104593493515932,
            1.1059289114384712,
            1.0963998557646064,
            1.091350941718379,
            1.0885185335255876,
            1.0825227732899823,
            1.0697950886774668,
            1.3784467389311972,
            1.3417239249507082,
            1.1485157525992091,
            1.1036415665964536,
            1.0786898596377312,
            1.0622891149943388,
            1.0536553603184373,
            1.044862025146243,
            1.0469408042823212,
            1.0428643053091025,
            1.0343302043178413,
            1.0266170758235305,
            1.0367577943620803,
            1.0216525815710236,
            1.0202594678613204,
            1.0175057906138747,
            1.0124849405469774,
            0.9993335755565499,
            0.9976604558244536,
            1.006601207618472,
            0.9902391184734393,
            0.9945328514787215,
            0.9919786076002484,
            0.9813875480543209,
            0.990074915976464,
            0.9742973959898646,
            0.9727111556861974,
            0.9687397231029559,
            0.973914501787741,
            0.9658128883265242,
            0.9732922066616106,
            0.9594361940516701,
            0.9523814350743837,
            0.9462103187283383,
            0.9473920645593088,
            0.9479771150818354,
            0.9423518354379679,
            0.9421454245531107,
            0.9420443388480174,
            0.9380591881426075,
            0.9247878804991517,
            0.9314463515824909,
            0.9154342722289169,
            0.9242821916749205,
            0.9145315855364257,
            0.9119443101218984,
            0.8996460301966607,
            0.9046983892404581,
            0.9028600099720533,
            0.8941048594969737,
            0.892935366570195,
            0.8968356296985964,
            0.8931825809840914,
            0.8863655555097363,
            0.8865997240513186,
            0.8737225034568883,
            0.8725759296477595,
            0.8825129496900341,
            0.8720255531842196,
            0.8716524356528174,
            0.873908926414538,
            0.8650844534741172,
            0.8540055178388765,
            0.852465587326243,
            0.845202819456028,
            0.8588660050042068,
            0.8742189437528199,
            0.8441137580931941,
            0.8378269234790078,
            0.8365559623211245,
            0.8311055725133871,
            0.8284198151359076,
            0.829531499856635,
            0.8376463447945027,
            0.8279350814940054,
            0.8182597311237191,
            0.815787313859674,
            0.8292848249024982,
            0.8150045328502413,
            0.8078308392174637,
            0.8061544110503378,
            0.7974804398379748,
            0.7981118431574181,
            0.7958171918422361,
            0.7936092532133754,
            0.7891728862931456,
            0.7836682728574246,
            0.7850555347490914,
            0.7858572896522812,
            0.776645640783672,
            0.7829527024981342,
            0.7726197280461276,
            0.7723646156395538,
            0.7666069518161726,
            0.776098288312743,
            0.7640378814709338,
            0.7626806075059915,
            0.759730689887759,
            0.7561388981493213,
            0.7676716914659814,
            0.7541511903835248,
            0.7517306118071834,
            0.7493740576732008,
            0.7525282917143423,
            0.7484342486043519,
            0.7423397702506825,
            0.7344492289084422,
            0.7386490144307101,
            0.7339498476137089,
            0.7278259185296071,
            0.738372300999074,
            0.7291512534588198,
            0.7293326515185682,
            0.7267369635497467,
            0.7185387000252929,
            0.7152698153181921,
            0.7213179967071437,
            0.7125460588479344,
            0.71210630634163,
            0.7084233647660364,
            0.708374646645558,
            0.7042658902421782,
            0.7025161355356627,
            0.7145586549481259,
            0.6967202206201191,
            0.6967122962203207,
            0.6923554524590697,
            0.6964213780209988,
            0.6974076839941966,
            0.6921646866617324,
            0.6899355234979074,
            0.6770515758779985,
            0.6858053426199322,
            0.6730278119256224,
            0.6756675190563444,
            0.6821189749089978,
            0.6723955862129791,
            1.4530204768422283,
            1.381950215448307,
            1.987076388129705,
            1.9105822572225257,
            1.3271752113028417,
            1.0869385766077646,
            0.9607806598083882,
            0.9130591229547428,
            0.8552755343763134,
            0.8397441264949267,
            0.8073823127565505,
            0.7862359602240068,
            0.769104224971578,
            0.7511139112182811,
            0.738114093677907,
            0.731443006026594,
            0.7284399286101136,
            0.7139300553104545,
            0.7089369493194774,
            0.7038086071799073,
            0.6854125585737108,
            0.6937331300747546,
            0.6792756809463983,
            0.6768484440030931,
            0.6753381143642377,
            0.6710476105726217,
            0.6694619723513157,
            0.6700863785381559,
            0.6689110141766222,
            0.6561060329026813,
            0.6523067053360275,
            0.6545474310464496,
            0.6499593703052665,
            0.6469217350211325,
            0.6444422964808307,
            0.6432002401804622,
            0.6589891782289818,
            0.6577570000781289,
            0.6437230479868152,
            0.6406142160862307,
            0.6339557321765755,
            0.6319500849216799,
            0.6266551779795296,
            0.6364323444004301,
            0.6309240558479405,
            0.622457391853574,
            0.6238415618486042,
            0.619608531269846,
            0.6208653615999825
        ],
        "eval": [
            1.2069,
            1.17463,
            1.17738,
            1.17239,
            1.18439,
            1.17238,
            1.17322,
            1.16122,
            1.14954,
            1.16218,
            1.1586,
            1.18106,
            1.14284,
            1.16379,
            1.20757,
            1.24872,
            1.17199,
            1.13223,
            1.1366,
            1.11127,
            1.10462,
            1.10041,
            1.0938,
            1.09599,
            1.09476,
            1.08898,
            1.08393,
            1.07588,
            1.09304,
            1.07726,
            1.0733,
            1.06901,
            1.07488,
            1.05918,
            1.06435,
            1.05747,
            1.05617,
            1.05377,
            1.04944,
            1.05822,
            1.03613,
            1.05189,
            1.0264,
            1.07803,
            1.02128,
            1.02225,
            1.02726,
            1.01061,
            1.01514,
            1.00854,
            0.99884,
            1.00569,
            1.01802,
            1.00528,
            0.99218,
            0.98107,
            0.98501,
            0.98495,
            0.97773,
            0.98696,
            0.96334,
            0.97263,
            0.96681,
            0.97405,
            0.97238,
            0.95132,
            0.95895,
            0.962,
            0.93881,
            0.94904,
            0.94192,
            0.9386,
            0.9373,
            0.93366,
            0.93455,
            0.91872,
            0.92405,
            0.92205,
            0.93077,
            0.9585,
            0.90912,
            0.92525,
            0.9113,
            0.90696,
            0.89714,
            0.91519,
            0.90953,
            0.90487,
            0.89785,
            0.89148,
            0.8986,
            0.90966,
            0.89777,
            0.88644,
            0.8852,
            0.86917,
            0.87291,
            0.87804,
            0.86272,
            0.86944,
            0.84992,
            0.86946,
            0.85095,
            0.86939,
            0.85138,
            0.85513,
            0.84775,
            0.83219,
            0.83965,
            0.83276,
            0.84564,
            0.83405,
            0.85009,
            0.82713,
            0.82224,
            0.83366,
            0.82901,
            0.81935,
            0.82796,
            0.82483,
            0.81283,
            0.82131,
            0.80927,
            0.80821,
            0.80934,
            0.81827,
            0.80492,
            0.79545,
            0.80853,
            0.80058,
            0.80676,
            0.79153,
            0.79807,
            0.79388,
            0.79745,
            0.79812,
            0.79924,
            0.77756,
            0.77903,
            0.79062,
            0.77035,
            0.78452,
            0.76341,
            0.7888,
            0.76281,
            0.7755,
            0.76231,
            0.75812,
            0.77518,
            0.75344,
            0.76447,
            1.42185,
            1.3148,
            1.70962,
            1.41153,
            1.18368,
            1.01611,
            0.96014,
            0.92324,
            0.88763,
            0.86943,
            0.84421,
            0.82767,
            0.81456,
            0.80494,
            0.80377,
            0.78611,
            0.78691,
            0.78362,
            0.76178,
            0.76383,
            0.76487,
            0.75835,
            0.75153,
            0.74681,
            0.74987,
            0.74876,
            0.74921,
            0.76089,
            0.74404,
            0.74446,
            0.73942,
            0.7327,
            0.72961,
            0.73294,
            0.73331,
            0.71891,
            0.72792,
            0.72546,
            0.71707,
            0.71599,
            0.73002,
            0.71401,
            0.71837,
            0.70719,
            0.71072,
            0.72708,
            0.70024,
            0.7023,
            0.71729
        ]
    },
    "architecture": "OCR_ViTRNN(\n  (batch_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 128, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n  )\n  (decoder): BiLSTMImageDecoder(\n    (norm): BatchNorm1d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (rnn): LSTM(128, 128, num_layers=2, dropout=0.1, bidirectional=True)\n    (out_proj): Linear(in_features=256, out_features=65, bias=True)\n  )\n  (softmax): LogSoftmax(dim=-1)\n)"
}