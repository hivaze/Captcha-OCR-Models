{
    "best_epoch": {
        "number": 199,
        "train_loss": 1.1318448019933096,
        "eval_loss": 1.19293
    },
    "history": {
        "train": [
            8.539476998244659,
            4.610996288589284,
            4.570891977865485,
            4.510913957523394,
            4.468658537804326,
            4.431269494793083,
            4.413335824314552,
            4.400933169111421,
            4.390138589883152,
            4.394655269912526,
            4.386508893363083,
            4.371907566167131,
            4.36317678041096,
            4.352925161772136,
            4.347023613845246,
            4.33781721622129,
            4.331581375266932,
            4.31970479820348,
            4.306288876110995,
            4.291466809526274,
            4.276433099674273,
            4.261107674127893,
            4.230525246149377,
            4.203801813004892,
            4.179752156704287,
            4.1475917598869225,
            4.120695681511601,
            4.0926120069962515,
            4.06802710400352,
            4.053289033189604,
            4.0177941925918,
            3.9845428979849515,
            3.9523157831988756,
            3.9141264655922035,
            3.8865597610232197,
            3.8521907631354995,
            3.8230064911178396,
            3.777155556256258,
            3.7486079795451106,
            3.7127359667910804,
            3.6759220829492882,
            3.629915225354931,
            3.5925051683112037,
            3.5580379721484605,
            3.509688326075107,
            3.479449700705613,
            3.446887650067293,
            3.402533238447165,
            3.3708678740489333,
            3.322316275367254,
            3.2940192645109154,
            3.26227066184901,
            3.2159958700590496,
            3.1824469898320453,
            3.1470177867744544,
            3.1285879038557223,
            3.081336784966384,
            3.0517174925985215,
            3.0128804369817805,
            2.9817035982880413,
            2.941229183462602,
            2.9234144114240816,
            2.8894288449347774,
            2.8621298783942115,
            2.822637331636646,
            2.7912565484831604,
            2.7657044959973685,
            2.7251311012461215,
            2.706112300293355,
            2.6820044155362286,
            2.6482966217813613,
            2.6172468028491056,
            2.597312447390979,
            2.572273369076886,
            2.54141936724699,
            2.538559325133698,
            2.522761133652699,
            2.482433786875085,
            2.447482051728647,
            2.4138832092285156,
            2.4026986104023607,
            2.378875512111036,
            2.356968592993821,
            2.335925669609746,
            2.320613676988626,
            2.295622065097471,
            2.2673917541021034,
            2.254964647413809,
            2.2451534965370277,
            2.2120829081233544,
            2.19808519942851,
            2.1720821631105642,
            2.161029690428625,
            2.130905256995672,
            2.1185487282427053,
            2.104834055598778,
            2.082875977588605,
            2.064155477511732,
            2.0547327950030945,
            2.0403364081925983,
            2.0288218546517287,
            2.0082441734362253,
            1.978588117828852,
            1.965618891052053,
            1.957268886928317,
            1.9286987419369854,
            1.9183912413029731,
            1.9015056649340858,
            1.8943958840792692,
            1.8752917730355565,
            1.8684199490124667,
            1.8491838581954376,
            1.837713279301607,
            1.8222337116169025,
            1.8120705643786659,
            1.7833217231533196,
            1.77764010731178,
            1.7711441411247737,
            1.7657302104974095,
            1.7466999232014524,
            1.7329820319067073,
            1.7195869548411309,
            1.7135270743430415,
            1.7161249118515207,
            1.7074264031422288,
            1.6773389592955383,
            1.6548425155349924,
            1.6434073161475267,
            1.6439894724495803,
            1.631062708323515,
            1.6297634492946576,
            1.6037962617753427,
            1.6010838022714928,
            1.5918937001047255,
            1.584775911101812,
            1.5775756081448327,
            1.5726746124557303,
            1.5578416812268994,
            1.5450152732148956,
            1.554784370374076,
            1.5240432024002075,
            1.5229535480088825,
            1.5302521729771095,
            1.5145159386381317,
            1.4939734301989591,
            1.4783938595011263,
            1.4778381902960283,
            1.4681295548813253,
            1.4541012848479837,
            1.4544561935376517,
            1.4358196801777128,
            1.4341093558299391,
            1.4283021356486068,
            1.4093793917305861,
            1.4043345632432382,
            1.4041736291933664,
            1.403216454047191,
            1.3904957273338414,
            1.3880563023724133,
            1.3710762126536309,
            1.3709983599336841,
            1.3524325286285788,
            1.359505525118188,
            1.3645101257517367,
            1.3352971816364723,
            1.3244298824781104,
            1.3180288151849675,
            1.3074811304671854,
            1.309502215325078,
            1.3055291658715358,
            1.3019615804092795,
            1.2931678340404849,
            1.2952918840360037,
            1.2753648199612582,
            1.2683780480034743,
            1.272740139236933,
            1.2628247239921666,
            1.2489174649685244,
            1.2553559677510322,
            1.248564899722232,
            1.2373814099951634,
            1.2283691258370122,
            1.2319433040256742,
            1.227677683287029,
            1.2253733541392073,
            1.206895671313322,
            1.2062911489341832,
            1.202836942069138,
            1.1932320051555392,
            1.1940960944453372,
            1.18210721166828,
            1.1763760756842698,
            1.1634590165524543,
            1.173790788348717,
            1.1649485969845252,
            1.1587138900273963,
            1.1514875903914246,
            1.1542207591141327,
            1.1505472222460975,
            1.1318448019933096
        ],
        "eval": [
            4.62981,
            4.60602,
            4.53658,
            4.4892,
            4.44674,
            4.42127,
            4.41827,
            4.39783,
            4.38775,
            4.39513,
            4.38305,
            4.37138,
            4.36806,
            4.34965,
            4.34867,
            4.3512,
            4.31766,
            4.3132,
            4.29927,
            4.29113,
            4.2689,
            4.24178,
            4.21787,
            4.18357,
            4.15771,
            4.12666,
            4.10192,
            4.09004,
            4.04598,
            4.04368,
            3.98923,
            3.97377,
            3.92279,
            3.90354,
            3.86236,
            3.85739,
            3.79621,
            3.77486,
            3.71637,
            3.67498,
            3.63923,
            3.59205,
            3.56334,
            3.51312,
            3.46212,
            3.46144,
            3.39759,
            3.35456,
            3.3366,
            3.28961,
            3.26828,
            3.20654,
            3.17348,
            3.14009,
            3.1119,
            3.0788,
            3.07162,
            3.02956,
            2.99559,
            2.94246,
            2.94817,
            2.88997,
            2.86495,
            2.92452,
            2.7886,
            2.75856,
            2.72841,
            2.72134,
            2.70221,
            2.64178,
            2.62831,
            2.59948,
            2.57186,
            2.55984,
            2.5197,
            2.53275,
            2.48912,
            2.44701,
            2.44445,
            2.40626,
            2.38224,
            2.36033,
            2.33686,
            2.31936,
            2.31697,
            2.26964,
            2.25842,
            2.24266,
            2.25129,
            2.19689,
            2.1829,
            2.20025,
            2.15709,
            2.12514,
            2.11245,
            2.10401,
            2.07039,
            2.06994,
            2.08549,
            2.03886,
            2.02161,
            1.98713,
            1.97884,
            1.96214,
            1.96238,
            1.94132,
            1.94899,
            1.89907,
            1.89416,
            1.89893,
            1.86426,
            1.85508,
            1.86484,
            1.82886,
            1.82691,
            1.79982,
            1.81534,
            1.80452,
            1.78037,
            1.75544,
            1.75025,
            1.75934,
            1.72605,
            1.72304,
            1.7132,
            1.69614,
            1.68129,
            1.67736,
            1.6601,
            1.66017,
            1.63688,
            1.6548,
            1.63187,
            1.63389,
            1.60933,
            1.59553,
            1.59256,
            1.59281,
            1.59143,
            1.55123,
            1.55292,
            1.56394,
            1.5402,
            1.54142,
            1.53286,
            1.53592,
            1.49746,
            1.48194,
            1.48231,
            1.48493,
            1.47225,
            1.48535,
            1.45335,
            1.45178,
            1.45235,
            1.45354,
            1.43595,
            1.42607,
            1.42829,
            1.42194,
            1.41474,
            1.39602,
            1.41967,
            1.38741,
            1.3717,
            1.38281,
            1.35522,
            1.35715,
            1.36039,
            1.34298,
            1.33926,
            1.32872,
            1.32142,
            1.31089,
            1.31234,
            1.32683,
            1.30242,
            1.30825,
            1.30386,
            1.2891,
            1.30183,
            1.28455,
            1.28818,
            1.28248,
            1.26438,
            1.2397,
            1.26456,
            1.2483,
            1.27222,
            1.24018,
            1.23288,
            1.24749,
            1.22102,
            1.23051,
            1.21403,
            1.21592,
            1.20989,
            1.20119,
            1.20906,
            1.19293
        ]
    },
    "architecture": "OCR_ViTRNN(\n  (batch_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 128, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=False)\n              (key): Linear(in_features=128, out_features=128, bias=False)\n              (value): Linear(in_features=128, out_features=128, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layernorm_before): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n  )\n  (decoder): BiLSTMImageDecoder(\n    (norm): BatchNorm1d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (rnn): LSTM(128, 128, num_layers=2, dropout=0.1, bidirectional=True)\n    (out_proj): Linear(in_features=256, out_features=65, bias=True)\n  )\n  (softmax): LogSoftmax(dim=-1)\n)"
}